{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **A Multi-Label Dataset of French Fake News**\n",
    "\n",
    "This NLP project was carried out as part of the *\"Machine Learning for NLP\"* course in the second year of the master's program at ENSAE. It is based on the paper [**A Multi-Label Dataset of French Fake News: Human and Machine Insights**](https://arxiv.org/abs/2403.16099) by Icard et al. (2024), and uses the associated GitHub repository [**OBSINFOX**](https://github.com/obs-info/obsinfox), which provides both the dataset used in this study and its accompanying documentation.\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset Overview — OBSINFOX\n",
    "\n",
    "* **Labels**: 11 distinct labels annotated for each document, with detailed definitions provided in the paper and repository README.\n",
    "* **Metadata**: title, annotator ID, article URL.\n",
    "* **Sources**: The dataset includes articles from **17 French media sources** identified as unreliable by watchdog organizations such as *NewsGuard* and *Conspiracy Watch*.\n",
    "\n",
    "The dataset consists of **100 documents** carefully selected from the aforementioned sources using a specific methodology, which will be explained later in the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Dataset Matters for Fake News Detection\n",
    "\n",
    "This dataset provides a rich foundation for studying how linguistic cues can signal that a text is **counterfactual**, **subjective**, or **satirical**. Several key aspects make it particularly valuable:\n",
    "\n",
    "* Each document is annotated along **11 complementary dimensions**, encompassing linguistic, stylistic, and factual properties.\n",
    "* Annotations were conducted by a panel of **8 expert annotators**, ensuring high-quality and nuanced evaluation.\n",
    "* The multi-label structure enables **multi-dimensional analysis** of fake news, capturing not only factual inaccuracies but also subtleties of tone, exaggeration, and bias.\n",
    "\n",
    "As such, OBSINFOX serves as a powerful resource for training and evaluating models capable of detecting weak signals of misinformation and editorial bias in French-language news content.\n",
    "\n",
    "---\n",
    "\n",
    "### *Importing Required Packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing the OBSINFOX Dataset**\n",
    "\n",
    "In this section, we begin by importing the OBSINFOX dataset and applying an initial round of preprocessing to prepare the data for analysis and modeling.\n",
    "\n",
    "The preprocessing pipeline will include the following steps:\n",
    "\n",
    "1. **Loading the dataset**: Reading the structured data and metadata from the OBSINFOX repository.\n",
    "\n",
    "2. **Text cleaning**: Removing unnecessary whitespace, special characters, and correcting encoding issues if present.\n",
    "\n",
    "3. **Normalization**: Lowercasing text and standardizing punctuation to reduce vocabulary size.\n",
    "\n",
    "4. **Exploratory filtering**: Ensuring the dataset is consistent and complete by removing empty or malformed entries.\n",
    "\n",
    "\n",
    "This stage is essential to ensure the quality and consistency of the textual data before moving on to any linguistic analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Fake News</th>\n",
       "      <th>Places, Dates, People</th>\n",
       "      <th>Facts</th>\n",
       "      <th>Opinions</th>\n",
       "      <th>Subjective</th>\n",
       "      <th>Reported information</th>\n",
       "      <th>Sources Cited</th>\n",
       "      <th>False Information</th>\n",
       "      <th>Insinuation</th>\n",
       "      <th>Exaggeration</th>\n",
       "      <th>Offbeat Title</th>\n",
       "      <th>Annotator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://lesakerfrancophone.fr/la-relation-entr...</td>\n",
       "      <td>La relation entre la technologie et la religion</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>rater1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.breizh-info.com/2021/01/27/157958/...</td>\n",
       "      <td>Confinement. Les habitants de Brest, Morlaix e...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>rater1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://reseauinternational.net/la-chine-le-pr...</td>\n",
       "      <td>La Chine : Le premier marché mondial de Smartp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>rater1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://lezarceleurs.blogspot.com/2021/12/emma...</td>\n",
       "      <td>Emmanuel à Olivier : « Tiens bon, on les aura ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>rater1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://lesakerfrancophone.fr/selon-ubs-les-pr...</td>\n",
       "      <td>Selon UBS, les « propriétés d’assurance tant d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>rater1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://lesakerfrancophone.fr/la-relation-entr...   \n",
       "1  https://www.breizh-info.com/2021/01/27/157958/...   \n",
       "2  https://reseauinternational.net/la-chine-le-pr...   \n",
       "3  https://lezarceleurs.blogspot.com/2021/12/emma...   \n",
       "4  https://lesakerfrancophone.fr/selon-ubs-les-pr...   \n",
       "\n",
       "                                               Title  Fake News  \\\n",
       "0    La relation entre la technologie et la religion          0   \n",
       "1  Confinement. Les habitants de Brest, Morlaix e...          0   \n",
       "2  La Chine : Le premier marché mondial de Smartp...          0   \n",
       "3  Emmanuel à Olivier : « Tiens bon, on les aura ...          1   \n",
       "4  Selon UBS, les « propriétés d’assurance tant d...          0   \n",
       "\n",
       "   Places, Dates, People  Facts  Opinions  Subjective  Reported information  \\\n",
       "0                      1      1         1           1                     0   \n",
       "1                      1      1         0           0                     0   \n",
       "2                      1      1         0           0                     0   \n",
       "3                      1      1         1           1                     0   \n",
       "4                      1      1         1           1                     0   \n",
       "\n",
       "   Sources Cited  False Information  Insinuation  Exaggeration  Offbeat Title  \\\n",
       "0              1                  0            0             0              0   \n",
       "1              1                  0            0             0              0   \n",
       "2              1                  0            0             0              0   \n",
       "3              0                  1            0             1              0   \n",
       "4              1                  0            0             0              0   \n",
       "\n",
       "  Annotator  \n",
       "0    rater1  \n",
       "1    rater1  \n",
       "2    rater1  \n",
       "3    rater1  \n",
       "4    rater1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('obsinfox.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Metadonnées & Labels*\n",
    "\n",
    "Chaque ligne du dataset contient les différentes annotations réalisées pour un texte et un annotateur. La construction du dataset inclut des métadonnées visant à donner du contexte aux différents textes : titre, URL, ainsi que l'ID anonyme de l'annotateur. Il y a donc 800 lignes dans le dataset correspondant aux 100 textes annotés par 8 personnes. \n",
    "\n",
    "The OBSINFOX dataset includes eleven distinct labels that capture various **factual**, **stylistic**, and **interpretative** features of news articles. These labels were designed to reflect both the objective content and the subjective framing often present in fake or misleading news. Below is a description of each label:\n",
    "\n",
    "\n",
    "* **Fake News**\n",
    "  The article contains at least one false or exaggerated fact.\n",
    "\n",
    "* **Places, Dates, People**\n",
    "  The article refers to at least one identifiable place, date, or person.\n",
    "\n",
    "* **Facts**\n",
    "  The article reports at least one factual element — that is, a state of affairs or event, whether true or false.\n",
    "\n",
    "* **Opinions**\n",
    "  The article expresses at least one opinion, judgment, or personal interpretation.\n",
    "\n",
    "* **Subjective**\n",
    "  The article contains more opinions than verifiable facts, highlighting a subjective tone or perspective.\n",
    "\n",
    "* **Reported Information**\n",
    "  The article relays information that is attributed to an external source and is not directly endorsed by the author.\n",
    "\n",
    "* **Sources Cited**\n",
    "  The article includes at least one cited source that supports or contextualizes a factual claim.\n",
    "\n",
    "* **False Information**\n",
    "  The article contains information that is demonstrably false or factually incorrect.\n",
    "\n",
    "* **Insinuation**\n",
    "  The article implies or suggests a certain interpretation without stating it explicitly.\n",
    "\n",
    "* **Exaggeration**\n",
    "  The article presents a real fact using language or framing that amplifies or distorts its significance.\n",
    "\n",
    "* **Offbeat Title**\n",
    "  The article has a misleading or sensational headline that does not accurately reflect the actual content.\n",
    "\n",
    "These labels aim to offer a nuanced characterization of the articles' content. Some of them are **objectively measurable** (e.g., *Places, Dates, People*, *Facts*, *Sources Cited*), while others are more **subject to interpretation**, reflecting the annotators’ perception and judgment.\n",
    "\n",
    "Studying the **co-occurrence patterns** among these labels provides valuable insight into how alternative or misleading news content is structured and perceived by human readers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DataFrame Summary\n",
      "========================================\n",
      "Number of rows    : 800\n",
      "Number of columns : 14\n",
      "\n",
      " Column List: \n",
      "\n",
      " 1. URL\n",
      " 2. Title\n",
      " 3. Fake News\n",
      " 4. Places, Dates, People\n",
      " 5. Facts\n",
      " 6. Opinions\n",
      " 7. Subjective\n",
      " 8. Reported information\n",
      " 9. Sources Cited\n",
      "10. False Information\n",
      "11. Insinuation\n",
      "12. Exaggeration\n",
      "13. Offbeat Title\n",
      "14. Annotator\n"
     ]
    }
   ],
   "source": [
    "print(\" DataFrame Summary\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Number of rows    : {df.shape[0]}\")\n",
    "print(f\"Number of columns : {df.shape[1]}\")\n",
    "print(\"\\n Column List: \\n\")\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1:>2}. {col}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du fait de sa petite taille, les données dans le dataset sont très qualitatives avec aucune valeur manquante et aucune duplication de lignes, ce qui facilite l'étape de nettoyage dans le preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL                      0\n",
       "Title                    0\n",
       "Fake News                0\n",
       "Places, Dates, People    0\n",
       "Facts                    0\n",
       "Opinions                 0\n",
       "Subjective               0\n",
       "Reported information     0\n",
       "Sources Cited            0\n",
       "False Information        0\n",
       "Insinuation              0\n",
       "Exaggeration             0\n",
       "Offbeat Title            0\n",
       "Annotator                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir une facilité d'accès aux différentes annotations pour un article spécifique, on ajoute au dataframe une colonne ``article_ID`` qui identifie de manière unique les 100 textes présents ici. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_id'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On présente quelques manipulation typique que l'on peut faire sur le dataset à travers quelques exemples: \n",
    "\n",
    "- On peut faire la moyenne des annotations réalisées par un annotateur à des fins de comparaison (certains vont être plus critiques par exemple)\n",
    "- Pour un article donné, on peut comparer les annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Summary: Mean values by Annotator for selected columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fake News</th>\n",
       "      <th>Places, Dates, People</th>\n",
       "      <th>Facts</th>\n",
       "      <th>Opinions</th>\n",
       "      <th>Subjective</th>\n",
       "      <th>Reported information</th>\n",
       "      <th>Sources Cited</th>\n",
       "      <th>False Information</th>\n",
       "      <th>Insinuation</th>\n",
       "      <th>Exaggeration</th>\n",
       "      <th>Offbeat Title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annotator</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rater1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rater2</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rater3</th>\n",
       "      <td>0.64</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rater4</th>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rater5</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rater6</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rater7</th>\n",
       "      <td>0.29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rater8</th>\n",
       "      <td>0.44</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Fake News  Places, Dates, People  Facts  Opinions  Subjective  \\\n",
       "Annotator                                                                  \n",
       "rater1          0.21                   0.90   0.93      0.80        0.71   \n",
       "rater2          0.29                   0.94   0.99      0.93        0.74   \n",
       "rater3          0.64                   1.00   0.96      0.74        0.63   \n",
       "rater4          0.34                   1.00   1.00      0.87        0.52   \n",
       "rater5          0.47                   0.93   0.96      0.66        0.67   \n",
       "rater6          0.34                   0.75   0.93      0.72        0.60   \n",
       "rater7          0.29                   1.00   0.91      0.45        0.47   \n",
       "rater8          0.44                   1.00   0.99      0.66        0.48   \n",
       "\n",
       "           Reported information  Sources Cited  False Information  \\\n",
       "Annotator                                                           \n",
       "rater1                     0.14           0.75               0.08   \n",
       "rater2                     0.21           0.82               0.09   \n",
       "rater3                     0.14           0.68               0.46   \n",
       "rater4                     0.22           0.87               0.32   \n",
       "rater5                     0.76           0.76               0.22   \n",
       "rater6                     0.21           0.60               0.11   \n",
       "rater7                     0.66           0.78               0.24   \n",
       "rater8                     0.11           0.46               0.34   \n",
       "\n",
       "           Insinuation  Exaggeration  Offbeat Title  \n",
       "Annotator                                            \n",
       "rater1            0.22          0.37           0.03  \n",
       "rater2            0.24          0.44           0.21  \n",
       "rater3            0.46          0.61           0.14  \n",
       "rater4            0.54          0.42           0.16  \n",
       "rater5            0.73          0.53           0.17  \n",
       "rater6            0.42          0.35           0.04  \n",
       "rater7            0.43          0.35           0.01  \n",
       "rater8            0.59          0.52           0.08  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = df.columns[2:13]  \n",
    "grouped_means = df.groupby('Annotator')[cols].mean()\n",
    "\n",
    "print(\"Mean values by Annotator for selected columns\")\n",
    "display(grouped_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_means.T.plot(kind='bar', figsize=(12, 6))\n",
    "\n",
    "plt.title(\"📊 Mean Annotation Scores by Annotator\")\n",
    "plt.xlabel(\"Annotation Labels\")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Annotator\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### **Statistiques Descriptives**\n",
    "\n",
    "Afin de nous approprier les données et avant d'aller plus dans l'analyse, nous allons sortir une série de statistiques pour décrire au mieux le dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Human Annotations*\n",
    "\n",
    "We now turn our attention to evaluating the quality and structure of the human annotations in the OBSINFOX dataset.\n",
    "\n",
    "Specifically, we aim to:\n",
    "\n",
    "* Assess the **consistency of the annotations** with expectations based on the dataset’s construction methodology,\n",
    "* Examine the **class balance** across the different labels,\n",
    "* Measure the **inter-annotator agreement**,\n",
    "* Analyze the **correlation between labels** to better understand their relationships.\n",
    "\n",
    "This analysis is a crucial step in our study, as it allows us to evaluate the **reliability and informativeness** of the labels provided. Understanding how human annotators identify fake news — and which linguistic or stylistic cues they rely on — will guide the rest of our project.\n",
    "\n",
    "In particular, this insight will help us identify:\n",
    "\n",
    "* Which labels are the most **discriminative** or **informative** for fake news detection,\n",
    "* How closely machine learning models can replicate or differ from **human reasoning**.\n",
    "\n",
    "Later in the project, we will compare these human-driven patterns with the predictions and internal logic of automated models, bridging the gap between human and machine interpretations of misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Topic & Genre Analysis de OBSINFOX**\n",
    "\n",
    "Aller sur le site internet pour générer une free API key et faire l'analyse de TOPIC / GENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'document.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sg/lxw89kss0x727nlhml9vq0nm0000gn/T/ipykernel_39304/2628502896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m files = {\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m'input'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'document.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'document.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'document.txt'"
     ]
    }
   ],
   "source": [
    "api_key = \"YOUR_GATE_CLOUD_API_KEY\"\n",
    "url = \"https://cloud.gate.ac.uk/process-document/gatecloud-service-name\"\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/x-yaml\",\n",
    "    \"GATECloud-API-Key\": api_key\n",
    "}\n",
    "\n",
    "files = {\n",
    "    'input': ('document.txt', open('document.txt', 'rb'))\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, files=files)\n",
    "result = response.json()\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Understanding Human vs Machine Caracterisation of Fake News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b620d6c80b6f496dbe7eeb17201993a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1306a6b81e9e47a38a4eb5b1665a8a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88838ed4535a41f79cddaf2ed9d67d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ec74e166d04ae28a30220b1377c210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mextract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_tiktoken_bpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting from Tiktoken\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m             return TikTokenConverter(\n\u001b[0m\u001b[1;32m   1728\u001b[0m                 \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mconverted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1625\u001b[0m         tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m         \u001b[0mvocab_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_vocab_merges_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_unk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mextract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1590\u001b[0m                 \u001b[0;34m\"`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sg/lxw89kss0x727nlhml9vq0nm0000gn/T/ipykernel_39304/2019954727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"VAGOsolutions/SauerkrautLM-7b-HerO\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0;34mf\"Tokenizer class {tokenizer_class_candidate} does not exist or is not currently imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 )\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# Otherwise we have to be creative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2063\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2300\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2302\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2303\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m             logger.info(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from_slow\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"additional_special_tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tiktoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mslow_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             ).converted()\n\u001b[1;32m   1731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1733\u001b[0m                 \u001b[0;34mf\"Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                 \u001b[0;34mf\"with a SentencePiece tokenizer.model file.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"VAGOsolutions/SauerkrautLM-7b-HerO\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"Les dernières nouvelles sur l'économie mondiale indiquent que\"\n",
    "result = generator(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
